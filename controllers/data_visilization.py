import pandas as pd
import numpy as np
import os

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_PATH = os.path.join(BASE_DIR, '..', 'data', 'cleaned_data.csv')
data = pd.read_csv(DATA_PATH)

  
def generate_visualizations():
    data.drop(columns=['inflationrate','borrowercreditscore'], inplace=True, errors='ignore')    
    total_applicants = len(data) 
    average_loan_amount = round(data['loanamount'].mean(), 2)
    average_remaining_amount = round(data['remainingamount'].mean(), 2)
    numeric = data.select_dtypes(include=[np.number])
    corr = numeric.corr().round(2)
    correlation_matrix= {
        "labels": corr.columns.tolist(),
        "matrix": corr.values.tolist()
    }


    # --- Risk Distribution ---
    risk_counts = data['lossdefault'].value_counts()
    total = risk_counts.sum()
    riskDistribution = [
        {
            "risk": "Low Risk",
            "count": int(risk_counts.get("No Default", 0)),
            "percentage": round(risk_counts.get("No Default", 0) / total * 100, 1),
            "color": "#22c55e",
        },
        {
            "risk": "High Risk",
            "count": int(risk_counts.get("Default", 0)),
            "percentage": round(risk_counts.get("Default", 0) / total * 100, 1),
            "color": "#ef4444",
        },
    ]

    # --- Low Risk Rate ---
    low_risk_rate = round((data['lossdefault'] == "No Default").mean() * 100, 1)

    # --- Region Distribution ---
    region_distribution = (
        data["region"]
        .value_counts(normalize=True)
        .mul(100)
        .round(1)
        .reset_index()
        .rename(columns={"index": "region", "region": "percentage"})
        .to_dict(orient="records")
    )

    bins = [0, 500000, 1000000, 1500000, 2000000, 3000000, np.inf]
    labels = ["â‰¤500K", "1M", "1.5M", "2M", "3M", "3M+"]
    data["loan_bin"] = pd.cut(data["loanamount"], bins=bins, labels=labels, include_lowest=True)

    # Convert lossdefault to numeric for mean risk
    data["lossdefault_numeric"] = data["lossdefault"].map({"No Default": 0, "Default": 1})

    loan_group = data.groupby("loan_bin", observed=True).agg(
        applications=("loanamount", "count"),
        risk=("lossdefault_numeric", "mean")
    ).reset_index()

    loan_amount_vs_risk = []
    for _, row in loan_group.iterrows():
        label = row["loan_bin"]
        risk = None if pd.isna(row["risk"]) else round(row["risk"], 2)
        apps = int(row["applications"])
        loan_amount_vs_risk.append({
            "amount": label,
            "risk": risk,
            "applications": apps,
        })

    # --- Correlation Matrix ---
    numeric_cols = data.select_dtypes(include=[np.number])
    corr = numeric_cols.corr()["lossdefault_numeric"].drop("lossdefault_numeric").abs().sort_values(ascending=False)
    correlations = corr.head(6).reset_index()
    correlations.columns = ['factor', 'correlation']
    correlations["importance"] = (correlations["correlation"] * 100).round().astype(int)
    correlations["A"] = correlations["importance"]
    correlations["fullMark"] = 100
    risk_factors = correlations[["factor", "importance", "A", "fullMark"]].to_dict(orient="records")
    accuracy=87.8
    precision=84.1
    recall=82.7
    f1_score=83.4


    # --- Final Result ---
    return {
        "totalApplicants": total_applicants,
        "averageLoanAmount": average_loan_amount,
        "averageRemainingAmount": average_remaining_amount,
        "riskDistribution": riskDistribution,
        "lowRiskRate": low_risk_rate,
        "regionDistribution": region_distribution,
        "loanAmountVsRisk": loan_amount_vs_risk,
        "riskFactors": risk_factors,
        "correlations":correlation_matrix,
        "accuracy": accuracy,
        "precision": precision, 
        "recall": recall,
        "f1_score": f1_score
    }

